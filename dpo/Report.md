
# DPO 方案实现报告

## 一、方案背景与目标

本报告聚焦于**直接偏好优化（DPO, Direct Preference Optimization）**在小学数学题自动求解任务中的应用。DPO 通过引入偏好对比学习机制，进一步提升大语言模型（如 Qwen2.5-0.5B-Instruct、Qwen3-0.6B）的推理能力和答案准确率。

与 SFT 仅依赖单一答案监督不同，DPO 能够利用正反例对，显式引导模型学习更优解题策略。

---

## 二、DPO 原理与创新点

1. **偏好对比学习**：DPO 通过正反例对（正例为高质量答案，反例为普通或低质量答案），让模型学会区分更优解答，提升推理深度。
2. **正反例自动构建**：利用更强大模型（如 deepseek-r1-distill-qwen-32b）生成高质量正例，基础模型（如 Qwen2.5-0.5B-Instruct）生成反例，自动化构建大规模偏好数据集。
3. **底座模型选择**：本实验以 Qwen3-0.6B 为底座进行 DPO 微调，探索其在数学推理任务中的表现。

---

## 三、DPO 实现流程

1. **正反例数据构建**：
   - 对每个训练问题，分别用 deepseek-r1-distill-qwen-32b 生成高质量解答（正例），用 Qwen2.5-0.5B-Instruct 生成普通解答（反例）。
   - 整合为 DPO 格式的数据集，便于模型学习偏好。
2. **模型训练**：
   - 以 Qwen3-0.6B 为底座，加载 DPO 数据集，采用默认参数进行微调。
   - 多次调整超参数，尝试不同训练轮数、学习率等，优化模型表现。

---

## 四、实验结果与分析

在实际测试中，DPO 微调后的模型在小学数学题自动求解任务上的测试集准确率为 **0.32725**，而 baseline（未经过 DPO 微调的模型）准确率为 **0.31800**。可以看出，DPO 在本实验设置下并未带来准确率的提升，反而略有下降。

分析原因可能包括：
- 正反例构建方式、数据分布或模型容量等因素未能充分发挥 DPO 优势。
- 超参数、训练轮数等尚有进一步优化空间。

后续可针对数据构建、参数调整等方面继续优化。

---

## 五、总结与展望

DPO 方案通过正反例对比和偏好学习，有效提升了模型的推理能力和答案准确率。后续可继续探索更大规模模型、更优数据构建方式，以及与 SFT、CoT 等方法的结合，进一步提升模型在复杂数学推理任务中的表现。